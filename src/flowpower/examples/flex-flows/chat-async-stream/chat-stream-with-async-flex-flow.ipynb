{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream chat with async flex flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives** - Upon completing this tutorial, you should be able to:\n",
    "\n",
    "- Write LLM application using class based flex flow.\n",
    "- Use OpenAIModelConfiguration as class init parameter.\n",
    "- Use prompty to stream completions.\n",
    "- Convert the application into a async flow and batch run against multi lines of data.\n",
    "- Use classed base flow to evaluate the main flow and learn how to do aggregation.\n",
    "\n",
    "## 0. Install dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trace your application with promptflow\n",
    "\n",
    "Assume we already have a python program, which leverage prompty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import asyncio\n",
      "import os\n",
      "from pathlib import Path\n",
      "\n",
      "from promptflow.tracing import trace\n",
      "from promptflow.core import OpenAIModelConfiguration, Prompty\n",
      "\n",
      "BASE_DIR = Path(__file__).absolute().parent\n",
      "\n",
      "\n",
      "def log(message: str):\n",
      "    verbose = os.environ.get(\"VERBOSE\", \"false\")\n",
      "    if verbose.lower() == \"true\":\n",
      "        print(message, flush=True)\n",
      "\n",
      "\n",
      "class ChatFlow:\n",
      "    def __init__(\n",
      "        self, model_config: OpenAIModelConfiguration, max_total_token=1100\n",
      "    ):\n",
      "        self.model_config = model_config\n",
      "        self.max_total_token = max_total_token\n",
      "\n",
      "    @trace\n",
      "    async def __call__(\n",
      "        self, question: str = \"What is ChatGPT?\", chat_history: list = None\n",
      "    ) -> str:\n",
      "        \"\"\"Flow entry function.\"\"\"\n",
      "\n",
      "        prompty = Prompty.load(\n",
      "            source=BASE_DIR / \"chat.prompty\",\n",
      "            model={\"configuration\": self.model_config},\n",
      "        )\n",
      "\n",
      "        chat_history = chat_history or []\n",
      "        # Try to render the prompt with token limit and reduce the history count if it fails\n",
      "        while len(chat_history) > 0:\n",
      "            token_count = prompty.estimate_token_count(\n",
      "                question=question, chat_history=chat_history\n",
      "            )\n",
      "            if token_count > self.max_total_token:\n",
      "                chat_history = chat_history[1:]\n",
      "                log(\n",
      "                    f\"Reducing chat history count to {len(chat_history)} to fit token limit\"\n",
      "                )\n",
      "            else:\n",
      "                break\n",
      "\n",
      "        # output is a generator of string as prompty enabled stream parameter\n",
      "        for output in prompty(question=question, chat_history=chat_history):\n",
      "            yield output\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    from promptflow.tracing import start_trace\n",
      "\n",
      "    start_trace()\n",
      "    config = OpenAIModelConfiguration(\n",
      "        connection=\"open_ai_connection\", azure_deployment=\"gpt-4o\"\n",
      "    )\n",
      "    flow = ChatFlow(model_config=config)\n",
      "    result = flow(\"What's Azure Machine Learning?\", [])\n",
      "\n",
      "    # print result in stream manner\n",
      "    async def consume_result():\n",
      "        async for output in result:\n",
      "            print(output, end=\"\")\n",
      "            await asyncio.sleep(0.01)\n",
      "\n",
      "    asyncio.run(consume_result())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"flow.py\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `stream=true` is configured in the parameters of a prompt whose output format is text, promptflow sdk will return a generator type, which item is the content of each chunk.\n",
    "\n",
    "Reference openai doc on how to do it using plain python code: [how_to_stream_completions](https://cookbook.openai.com/examples/how_to_stream_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "name: Stream Chat\n",
      "description: Chat with stream enabled.\n",
      "model:\n",
      "  api: chat\n",
      "  configuration:\n",
      "    type: azure_openai\n",
      "    model: gpt-3.5-turbo\n",
      "  parameters:\n",
      "    temperature: 0.2\n",
      "    stream: true\n",
      "    max_tokens: 1024\n",
      "inputs: \n",
      "  question:\n",
      "    type: string\n",
      "  chat_history:\n",
      "    type: list\n",
      "sample:\n",
      "  question: \"What is Prompt flow?\"\n",
      "---\n",
      "\n",
      "system:\n",
      "You are a helpful assistant.\n",
      "\n",
      "{% for item in chat_history %}\n",
      "{{item.role}}:\n",
      "{{item.content}}\n",
      "{% endfor %}\n",
      "\n",
      "user:\n",
      "{{question}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"chat.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create necessary connections\n",
    "Connection helps securely store and manage secret keys or other sensitive credentials required for interacting with LLM and other external tools for example Azure Content Safety.\n",
    "\n",
    "Above prompty uses connection `open_ai_connection` inside, we need to set up the connection if we haven't added it before. After created, it's stored in local db and can be used in any flow.\n",
    "\n",
    "Prepare your Azure OpenAI resource follow this [instruction](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal) and get your `api_key` if you don't have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing connection\n",
      "name: open_ai_connection\n",
      "module: promptflow.connections\n",
      "created_date: '2025-04-21T12:39:40.203530'\n",
      "last_modified_date: '2025-04-21T14:34:50.329413'\n",
      "type: open_ai\n",
      "api_key: '******'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from promptflow.client import PFClient\n",
    "from promptflow.connections import AzureOpenAIConnection, OpenAIConnection\n",
    "\n",
    "# client can help manage your runs and connections.\n",
    "pf = PFClient()\n",
    "try:\n",
    "    conn_name = \"open_ai_connection\"\n",
    "    conn = pf.connections.get(name=conn_name)\n",
    "    print(\"using existing connection\")\n",
    "except:\n",
    "    # Follow https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal to create an Azure OpenAI resource.\n",
    "    connection = AzureOpenAIConnection(\n",
    "        name=conn_name,\n",
    "        api_key=\"<your_AOAI_key>\",\n",
    "        api_base=\"<your_AOAI_endpoint>\",\n",
    "        api_type=\"azure\",\n",
    "    )\n",
    "\n",
    "    # use this if you have an existing OpenAI account\n",
    "    # connection = OpenAIConnection(\n",
    "    #     name=conn_name,\n",
    "    #     api_key=\"<user-input>\",\n",
    "    # )\n",
    "\n",
    "    conn = pf.connections.create_or_update(connection)\n",
    "    print(\"successfully created connection\")\n",
    "\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trace by using start_trace\n",
    "\n",
    "Note we add `@trace` in the `my_llm_tool` function, re-run below cell will collect a trace in trace UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import OpenAIModelConfiguration\n",
    "\n",
    "model_config = OpenAIModelConfiguration(\n",
    "    connection=\"open_ai_connection\",\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<async_generator object ChatFlow.__call__ at 0x70de6c877340>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "# from promptflow.core import OpenAIModelConfiguration\n",
    "from promptflow.core import OpenAIModelConfiguration\n",
    "\n",
    "from flow import ChatFlow\n",
    "\n",
    "# ✅ Use plain OpenAI config instead of Azure\n",
    "config = OpenAIModelConfiguration(\n",
    "    connection=\"open_ai_connection\",  # must match the created connection name\n",
    "    model=\"gpt-3.5-turbo\",  # actual OpenAI model\n",
    ")\n",
    "chat_flow = ChatFlow(config)\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()\n",
    "\n",
    "# run the flow as function, which will be recorded in the trace\n",
    "result = chat_flow(question=\"What is ChatGPT? Please explain with detailed statement\")\n",
    "# note the type is async generator object as we enabled stream in prompty\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛰️ Sending request to OpenAI with params:\n",
      "{\n",
      "  \"temperature\": 0.2,\n",
      "  \"stream\": true,\n",
      "  \"max_tokens\": 1024,\n",
      "  \"model\": \"gpt-3.5-turbo\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a helpful assistant.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What is ChatGPT? Please explain with detailed statement\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "🔐 Client type: <class 'openai.OpenAI'>\n",
      "🔐 Client config keys: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_is_protocol', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'evals', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'responses', 'timeout', 'uploads', 'user_agent', 'vector_stores', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']\n",
      "Chatbot GPT (ChatGPT) is an advanced conversational AI model developed by OpenAI. It is based on the GPT-3 (Generative Pre-trained Transformer 3) architecture, which is a state-of-the-art language model capable of generating human-like text responses. ChatGPT is specifically fine-tuned for engaging in natural language conversations with users, providing responses that are contextually relevant and coherent.\n",
      "\n",
      "ChatGPT can understand and generate text in multiple languages and is designed to handle a wide range of topics and conversational styles. It can be used in various applications such as customer support chatbots, virtual assistants, language translation services, and more.\n",
      "\n",
      "Overall, ChatGPT represents a significant advancement in AI technology, enabling more natural and human-like interactions between machines and humans."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# print result in stream manner\n",
    "async for output in result:\n",
    "    print(output, end=\"\")\n",
    "    await asyncio.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛰️ Sending request to OpenAI with params:\n",
      "{\n",
      "  \"temperature\": 0.2,\n",
      "  \"stream\": true,\n",
      "  \"max_tokens\": 1024,\n",
      "  \"model\": \"gpt-3.5-turbo\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a helpful assistant.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What is ChatGPT? Please explain with consise statement\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "🔐 Client type: <class 'openai.OpenAI'>\n",
      "🔐 Client config keys: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_is_protocol', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'evals', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'responses', 'timeout', 'uploads', 'user_agent', 'vector_stores', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ChatGPT is an AI language model developed by OpenAI that can generate human-like text responses to prompts, enabling natural and engaging conversations with users.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat_flow(question=\"What is ChatGPT? Please explain with consise statement\")\n",
    "\n",
    "answer = \"\"\n",
    "async for output in result:\n",
    "    answer += output\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛰️ Sending request to OpenAI with params:\n",
      "{\n",
      "  \"max_tokens\": 256,\n",
      "  \"temperature\": 0.7,\n",
      "  \"model\": \"gpt-3.5-turbo\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are an AI assistant. \\nYou task is to evaluate a score based on how the statement applies for the answer.\\nOnly accepts valid JSON format response without extra prefix or postfix.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"This score value should always be an integer between 1 and 5. So the score produced should be 1 or 2 or 3 or 4 or 5.\\n\\nHere are a few examples:\\nanswer: ChatGPT is a conversational AI model developed by OpenAI.\\nstatement: It contains a brief explanation of ChatGPT.\\nOUTPUT:\\n{\\\"score\\\": \\\"5\\\", \\\"explanation\\\":\\\"The statement is correct. The answer contains a brief explanation of ChatGPT.\\\"}\\n\\nFor a given answer, valuate the answer based on how the statement applies for the answer:\\nanswer: ChatGPT is an AI language model developed by OpenAI that can generate human-like text responses to prompts, enabling natural and engaging conversations with users.\\nstatement: It contains a detailed explanation of ChatGPT.\\nOUTPUT:\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "🔐 Client type: <class 'openai.OpenAI'>\n",
      "🔐 Client config keys: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_is_protocol', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'evals', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'responses', 'timeout', 'uploads', 'user_agent', 'vector_stores', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']\n",
      "🛰️ Sending request to OpenAI with params:\n",
      "{\n",
      "  \"max_tokens\": 256,\n",
      "  \"temperature\": 0.7,\n",
      "  \"model\": \"gpt-3.5-turbo\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are an AI assistant. \\nYou task is to evaluate a score based on how the statement applies for the answer.\\nOnly accepts valid JSON format response without extra prefix or postfix.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"This score value should always be an integer between 1 and 5. So the score produced should be 1 or 2 or 3 or 4 or 5.\\n\\nHere are a few examples:\\nanswer: ChatGPT is a conversational AI model developed by OpenAI.\\nstatement: It contains a brief explanation of ChatGPT.\\nOUTPUT:\\n{\\\"score\\\": \\\"5\\\", \\\"explanation\\\":\\\"The statement is correct. The answer contains a brief explanation of ChatGPT.\\\"}\\n\\nFor a given answer, valuate the answer based on how the statement applies for the answer:\\nanswer: ChatGPT is an AI language model developed by OpenAI that can generate human-like text responses to prompts, enabling natural and engaging conversations with users.\\nstatement: It is a consise statement.\\nOUTPUT:\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "🔐 Client type: <class 'openai.OpenAI'>\n",
      "🔐 Client config keys: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_is_protocol', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'evals', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'responses', 'timeout', 'uploads', 'user_agent', 'vector_stores', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'correctness': {'score': '3',\n",
       "  'explanation': \"The statement is partially correct. The answer provides a detailed explanation of ChatGPT, but it does not explicitly mention 'detailed explanation' in the statement.\"},\n",
       " 'consise': {'score': '3'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import paths  # add the code_quality module to the path\n",
    "from check_list import EvalFlow\n",
    "\n",
    "eval_flow = EvalFlow(config)\n",
    "# evaluate answer agains a set of statement\n",
    "eval_result = eval_flow(\n",
    "    answer=answer,\n",
    "    statements={\n",
    "        \"correctness\": \"It contains a detailed explanation of ChatGPT.\",\n",
    "        \"consise\": \"It is a consise statement.\",\n",
    "    },\n",
    ")\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch run the function as flow with multi-line data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch run with a data file (with multiple lines of test data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import PFClient\n",
    "\n",
    "pf = PFClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-21 14:35:59 -0300][promptflow][WARNING] - Found existing /home/matias/repos/flowpower/src/flowpower/examples/flex-flows/chat-async-stream/flow.flex.yaml, will not respect it in runtime.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=chat_async_stream_20250421_143559_228684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-21 14:35:59 -0300][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run chat_async_stream_20250421_143559_228684, log path: /home/matias/.promptflow/.runs/chat_async_stream_20250421_143559_228684/logs.txt\n",
      "[2025-04-21 14:36:05 -0300][promptflow.core._prompty_utils][WARNING] - gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 14:36:08 -0300 1909783 execution.bulk     INFO     Process 1909956 terminated.\n",
      "2025-04-21 14:36:08 -0300 1909783 execution.bulk     WARNING  Process 1909964 had been terminated.\n",
      "2025-04-21 14:36:08 -0300 1909783 execution.bulk     WARNING  Process 1909972 had been terminated.\n",
      "2025-04-21 14:35:59 -0300 1901803 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-04-21 14:36:00 -0300 1901803 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3}.\n",
      "2025-04-21 14:36:03 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(1909956)-Line number(0) start execution.\n",
      "2025-04-21 14:36:03 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(1909964)-Line number(1) start execution.\n",
      "2025-04-21 14:36:03 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(1909972)-Line number(2) start execution.\n",
      "2025-04-21 14:36:04 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(1909964)-Line number(1) completed.\n",
      "2025-04-21 14:36:05 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(1909956)-Line number(0) completed.\n",
      "2025-04-21 14:36:05 -0300 1901803 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-21 14:36:05 -0300 1901803 execution.bulk     INFO     Average execution time for completed lines: 2.5 seconds. Estimated time for incomplete lines: 2.5 seconds.\n",
      "2025-04-21 14:36:07 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(1909972)-Line number(2) completed.\n",
      "2025-04-21 14:36:08 -0300 1901803 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-21 14:36:08 -0300 1901803 execution.bulk     INFO     Average execution time for completed lines: 2.67 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-04-21 14:36:08 -0300 1901803 execution.bulk     INFO     The thread monitoring the process [1909956-ForkProcess-4:1] will be terminated.\n",
      "2025-04-21 14:36:08 -0300 1901803 execution.bulk     INFO     The thread monitoring the process [1909964-ForkProcess-4:2] will be terminated.\n",
      "2025-04-21 14:36:08 -0300 1901803 execution.bulk     INFO     The thread monitoring the process [1909972-ForkProcess-4:3] will be terminated.\n",
      "2025-04-21 14:36:08 -0300 1909956 execution.bulk     INFO     The process [1909956] has received a terminate signal.\n",
      "2025-04-21 14:36:08 -0300 1909964 execution.bulk     INFO     The process [1909964] has received a terminate signal.\n",
      "2025-04-21 14:36:08 -0300 1909972 execution.bulk     INFO     The process [1909972] has received a terminate signal.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"chat_async_stream_20250421_143559_228684\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-04-21 14:35:59.227038-03:00\"\n",
      "Duration: \"0:00:10.102477\"\n",
      "Output path: \"/home/matias/.promptflow/.runs/chat_async_stream_20250421_143559_228684\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = \"./data.jsonl\"  # path to the data file\n",
    "# create run with the flow function and data\n",
    "base_run = pf.run(\n",
    "    flow=chat_flow,\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"chat_history\": \"${data.chat_history}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.chat_history</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Prompt flow?</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Prompt flow refers to the sequence of prompts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is ChatGPT? Please explain with consise s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>ChatGPT is a conversational AI model developed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many questions did user ask?</td>\n",
       "      <td>[{'role': 'user', 'content': 'where is the nea...</td>\n",
       "      <td>2</td>\n",
       "      <td>You have asked two questions so far. How can I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     inputs.question  \\\n",
       "0                               What is Prompt flow?   \n",
       "1  What is ChatGPT? Please explain with consise s...   \n",
       "2                   How many questions did user ask?   \n",
       "\n",
       "                                 inputs.chat_history  inputs.line_number  \\\n",
       "0                                                 []                   0   \n",
       "1                                                 []                   1   \n",
       "2  [{'role': 'user', 'content': 'where is the nea...                   2   \n",
       "\n",
       "                                      outputs.output  \n",
       "0  Prompt flow refers to the sequence of prompts ...  \n",
       "1  ChatGPT is a conversational AI model developed...  \n",
       "2  You have asked two questions so far. How can I...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pf.get_details(base_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate your flow\n",
    "Then you can use an evaluation method to evaluate your flow. The evaluation methods are also flows which usually using LLM assert the produced output matches certain expectation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on the previous batch run\n",
    "The **base_run** is the batch run we completed in step 2 above, for web-classification flow with \"data.jsonl\" as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-21 14:36:09 -0300][promptflow][WARNING] - Found existing /home/matias/repos/flowpower/src/flowpower/examples/flex-flows/chat-async-stream/flow.flex.yaml, will not respect it in runtime.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=check_list_evalflow_b3voou12_20250421_143609_676623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-21 14:36:09 -0300][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run check_list_evalflow_b3voou12_20250421_143609_676623, log path: /home/matias/.promptflow/.runs/check_list_evalflow_b3voou12_20250421_143609_676623/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 14:36:17 -0300 1910302 execution.bulk     INFO     Process 1910464 terminated.\n",
      "2025-04-21 14:36:17 -0300 1910302 execution.bulk     WARNING  Process 1910478 had been terminated.\n",
      "2025-04-21 14:36:17 -0300 1910302 execution.bulk     WARNING  Process 1910471 had been terminated.\n",
      "2025-04-21 14:36:10 -0300 1901803 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-04-21 14:36:10 -0300 1901803 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3}.\n",
      "2025-04-21 14:36:13 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-8:3)-Process id(1910478)-Line number(0) start execution.\n",
      "2025-04-21 14:36:13 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-8:1)-Process id(1910464)-Line number(1) start execution.\n",
      "2025-04-21 14:36:13 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-8:2)-Process id(1910471)-Line number(2) start execution.\n",
      "2025-04-21 14:36:15 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-8:1)-Process id(1910464)-Line number(1) completed.\n",
      "2025-04-21 14:36:15 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-8:2)-Process id(1910471)-Line number(2) completed.\n",
      "2025-04-21 14:36:16 -0300 1901803 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-21 14:36:16 -0300 1901803 execution.bulk     INFO     Average execution time for completed lines: 3.0 seconds. Estimated time for incomplete lines: 3.0 seconds.\n",
      "2025-04-21 14:36:16 -0300 1901803 execution.bulk     INFO     Process name(ForkProcess-8:3)-Process id(1910478)-Line number(0) completed.\n",
      "2025-04-21 14:36:17 -0300 1901803 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-21 14:36:17 -0300 1901803 execution.bulk     INFO     Average execution time for completed lines: 2.34 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-04-21 14:36:17 -0300 1901803 execution.bulk     INFO     The thread monitoring the process [1910464-ForkProcess-8:1] will be terminated.\n",
      "2025-04-21 14:36:17 -0300 1901803 execution.bulk     INFO     The thread monitoring the process [1910478-ForkProcess-8:3] will be terminated.\n",
      "2025-04-21 14:36:17 -0300 1901803 execution.bulk     INFO     The thread monitoring the process [1910471-ForkProcess-8:2] will be terminated.\n",
      "2025-04-21 14:36:17 -0300 1910464 execution.bulk     INFO     The process [1910464] has received a terminate signal.\n",
      "2025-04-21 14:36:17 -0300 1910478 execution.bulk     INFO     The process [1910478] has received a terminate signal.\n",
      "2025-04-21 14:36:17 -0300 1910471 execution.bulk     INFO     The process [1910471] has received a terminate signal.\n",
      "2025-04-21 14:36:18 -0300 1901803 execution.bulk     INFO     Executing aggregation function...\n",
      "2025-04-21 14:36:18 -0300 1901803 execution.bulk     INFO     Finish executing aggregation function.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"check_list_evalflow_b3voou12_20250421_143609_676623\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-04-21 14:36:09.674150-03:00\"\n",
      "Duration: \"0:00:08.449759\"\n",
      "Output path: \"/home/matias/.promptflow/.runs/check_list_evalflow_b3voou12_20250421_143609_676623\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_run = pf.run(\n",
    "    flow=eval_flow,\n",
    "    data=\"./data.jsonl\",  # path to the data file\n",
    "    run=base_run,  # specify base_run as the run you want to evaluate\n",
    "    column_mapping={\n",
    "        \"answer\": \"${run.outputs.output}\",\n",
    "        \"statements\": \"${data.statements}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.answer</th>\n",
       "      <th>inputs.statements</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.correctness</th>\n",
       "      <th>outputs.consise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prompt flow refers to the sequence of prompts ...</td>\n",
       "      <td>{'correctness': 'result should be 1', 'consise...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'score': '1', 'explanation': 'The statement i...</td>\n",
       "      <td>{'score': '3', 'explanation': 'The statement i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ChatGPT is a conversational AI model developed...</td>\n",
       "      <td>{'correctness': 'result should be 1', 'consise...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'score': '1', 'explanation': 'The statement i...</td>\n",
       "      <td>{'score': '3'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You have asked two questions so far. How can I...</td>\n",
       "      <td>{'correctness': 'result should be 1', 'consise...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'score': '1', 'explanation': 'The statement i...</td>\n",
       "      <td>{'score': '3', 'explanation': 'The statement i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       inputs.answer  \\\n",
       "0  Prompt flow refers to the sequence of prompts ...   \n",
       "1  ChatGPT is a conversational AI model developed...   \n",
       "2  You have asked two questions so far. How can I...   \n",
       "\n",
       "                                   inputs.statements  inputs.line_number  \\\n",
       "0  {'correctness': 'result should be 1', 'consise...                   0   \n",
       "1  {'correctness': 'result should be 1', 'consise...                   1   \n",
       "2  {'correctness': 'result should be 1', 'consise...                   2   \n",
       "\n",
       "                                 outputs.correctness  \\\n",
       "0  {'score': '1', 'explanation': 'The statement i...   \n",
       "1  {'score': '1', 'explanation': 'The statement i...   \n",
       "2  {'score': '1', 'explanation': 'The statement i...   \n",
       "\n",
       "                                     outputs.consise  \n",
       "0  {'score': '3', 'explanation': 'The statement i...  \n",
       "1                                     {'score': '3'}  \n",
       "2  {'score': '3', 'explanation': 'The statement i...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pf.get_details(eval_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"average_correctness\": 1.0,\n",
      "    \"total\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "metrics = pf.get_metrics(eval_run)\n",
    "print(json.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "The HTML file is generated at '/tmp/pf-visualize-detail-fhz1fves.html'.\n",
      "Trying to view the result in a web browser...\n",
      "Successfully visualized from the web browser.\n"
     ]
    }
   ],
   "source": [
    "pf.visualize([base_run, eval_run])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "By now you've successfully run your chat flow and did evaluation on it. That's great!\n",
    "\n",
    "You can check out more examples:\n",
    "- [Stream Chat](https://github.com/microsoft/promptflow/tree/main/examples/flex-flows/chat-stream): demonstrates how to create a chatbot that runs in streaming mode."
   ]
  }
 ],
 "metadata": {
  "build_doc": {
   "author": [
    "D-W-@github.com",
    "wangchao1230@github.com"
   ],
   "category": "local",
   "section": "Flow",
   "weight": 12
  },
  "description": "A quickstart tutorial to run a class based flex flow in stream mode and evaluate it.",
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "resources": "examples/requirements.txt, examples/flex-flows/chat-basic, examples/flex-flows/eval-checklist"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
